import asyncio
import os
import argparse
import logging
import logging.config
import inspect
import json
import shutil
import re
import base64
import requests
import time
import importlib
import sys
from pathlib import Path
from typing import List, Dict, Any, Optional, Union
from datetime import datetime
from dotenv import load_dotenv

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(str(Path(__file__).parent))

# MinerU 2.0 é…ç½® - ä¸å†éœ€è¦é…ç½®æ–‡ä»¶ï¼Œæ”¹ç”¨ç¯å¢ƒå˜é‡å’Œå‚æ•°é…ç½®
# è®¾ç½® MinerU æ¨¡å‹æºï¼ˆå¯é€‰ï¼šhuggingface, modelscope, localï¼‰
MINERU_MODEL_SOURCE = os.getenv("MINERU_MODEL_SOURCE", "local")
MINERU_DEVICE = os.getenv("MINERU_DEVICE", "cpu")  # cpu, cuda, cuda:0, npu, mps

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ["MINERU_MODEL_SOURCE"] = MINERU_MODEL_SOURCE

# æ£€æŸ¥ MinerU 2.0 å®‰è£…çŠ¶æ€
def check_mineru_installation():
    """æ£€æŸ¥ MinerU 2.0 æ˜¯å¦æ­£ç¡®å®‰è£…"""
    try:
        # å°è¯•å¯¼å…¥ MinerU æ ¸å¿ƒæ¨¡å—
        import mineru
        print(f"âœ… MinerU ç‰ˆæœ¬: {getattr(mineru, '__version__', 'æœªçŸ¥')}")
        return True
    except ImportError:
        print("âŒ MinerU æœªå®‰è£…")
        print("å®‰è£…å‘½ä»¤: pip install -U 'mineru[core]'")
        return False
    except Exception as e:
        print(f"âŒ MinerU æ£€æŸ¥å¤±è´¥: {e}")
        return False

# æ‰§è¡Œå®‰è£…æ£€æŸ¥
HAS_MINERU = check_mineru_installation()

# æ ¸å¿ƒå¯¼å…¥ - åŸºäºå››ä¸ªç¤ºä¾‹æ–‡ä»¶çš„å…³é”®æ¨¡å—
from lightrag import LightRAG, QueryParam
from lightrag.llm.ollama import ollama_model_complete, ollama_embed
from lightrag.utils import EmbeddingFunc, logger, set_verbose_debug
from lightrag.kg.shared_storage import initialize_pipeline_status

# å¯¼å…¥æœ¬åœ°æç¤ºè¯æ–‡ä»¶
import prompt as local_prompt

# å°è¯•å¯¼å…¥RAGAnythingç›¸å…³æ¨¡å—
try:
    from raganything import RAGAnything, RAGAnythingConfig
    from raganything.modalprocessors import (
        ImageModalProcessor,
        TableModalProcessor, 
        EquationModalProcessor,
    )
    HAS_RAGANYTHING = True
    print("âœ… RAGAnything æ¨¡å—å¯¼å…¥æˆåŠŸ")
except ImportError as e:
    HAS_RAGANYTHING = False
    print(f"âŒ RAGAnythingæœªå®‰è£…æˆ–å¯¼å…¥å¤±è´¥: {e}")
    print("æç¤º: è¯·è¿è¡Œ 'pip install raganything' å®‰è£…")

# åŠ è½½ç¯å¢ƒå˜é‡ - åªåŠ è½½ä¸€æ¬¡
load_dotenv(dotenv_path=".env", override=False)

# å…¨å±€é…ç½®
WORKING_DIR = "./financial_rag"
OUTPUT_DIR = "./financial_output"

# Ollamaæ¨¡å‹é…ç½® - ä»ç¯å¢ƒå˜é‡è¯»å–ï¼Œæ”¯æŒåŠ¨æ€é…ç½®
VISION_MODEL = os.getenv("VISION_MODEL", "qwen2.5vl:3b-q4_K_M")        # è§†è§‰ç†è§£
EMBEDDING_MODEL = os.getenv("EMBEDDING_MODEL", "bge-m3:latest")        # æ–‡æœ¬åµŒå…¥
RERANK_MODEL = os.getenv("RERANK_MODEL_LOCAL", "qllama/bge-reranker-large:latest")  # é‡æ’åº
EXTRACTION_MODEL = os.getenv("EXTRACTION_MODEL", "qwen3:1.7b-q4_K_M")  # æå–æ¨¡å‹
ANSWER_MODEL = os.getenv("ANSWER_MODEL", "qwen3:8b-q4_K_M")            # æœ€ç»ˆé—®ç­”æ¨¡å‹

# æ‰“å°æ¨¡å‹é…ç½®ä¿¡æ¯
print(f"\nç ”æŠ¥å¤„ç†æ¨¡å‹é…ç½®:")
print(f"  - è§†è§‰æ¨¡å‹: {VISION_MODEL}")
print(f"  - åµŒå…¥æ¨¡å‹: {EMBEDDING_MODEL}")
print(f"  - é‡æ’åºæ¨¡å‹: {RERANK_MODEL}")
print(f"  - æå–æ¨¡å‹: {EXTRACTION_MODEL}")
print(f"  - é—®ç­”æ¨¡å‹: {ANSWER_MODEL}")
if os.getenv("FORCE_MODEL"):
    print(f"  - å¼ºåˆ¶æ¨¡å‹: {os.getenv('FORCE_MODEL')}")
print()


# OllamaæœåŠ¡é…ç½®
OLLAMA_HOST = os.getenv("OLLAMA_HOST", "http://localhost:11434")
TIMEOUT = int(os.getenv("TIMEOUT", "300"))


def configure_logging():
    """é…ç½®æ—¥å¿—ç³»ç»Ÿ - åŸºäºraganything_example.pyçš„æ—¥å¿—é…ç½®"""
    log_dir = os.getenv("LOG_DIR", os.getcwd())
    log_file_path = os.path.abspath(os.path.join(log_dir, "financial_report_processor.log"))
    
    print(f"\né‡‘èæŠ¥å‘Šå¤„ç†å™¨æ—¥å¿—æ–‡ä»¶: {log_file_path}\n")
    os.makedirs(os.path.dirname(log_file_path), exist_ok=True)
    
    # ä»ç¯å¢ƒå˜é‡è·å–æ—¥å¿—é…ç½®
    log_max_bytes = int(os.getenv("LOG_MAX_BYTES", 10485760))  # é»˜è®¤10MB
    log_backup_count = int(os.getenv("LOG_BACKUP_COUNT", 5))   # é»˜è®¤5ä¸ªå¤‡ä»½
    
    logging.config.dictConfig({
        "version": 1,
        "disable_existing_loggers": False,
        "formatters": {
            "default": {
                "format": "%(levelname)s: %(message)s",
            },
            "detailed": {
                "format": "%(asctime)s - %(name)s - %(levelname)s - %(message)s",
            },
        },
        "handlers": {
            "console": {
                "formatter": "default",
                "class": "logging.StreamHandler",
                "stream": "ext://sys.stderr",
            },
            "file": {
                "formatter": "detailed",
                "class": "logging.handlers.RotatingFileHandler",
                "filename": log_file_path,
                "maxBytes": log_max_bytes,
                "backupCount": log_backup_count,
                "encoding": "utf-8",
            },
        },
        "loggers": {
            "lightrag": {
                "handlers": ["console", "file"],
                "level": "INFO",
                "propagate": False,
            },
        },
    })
    
    # è®¾ç½®æ—¥å¿—çº§åˆ«
    logger.setLevel(logging.INFO)
    set_verbose_debug(True)  

async def ollama_vision_complete_direct(prompt: str, system_prompt: str = None, image_data: str = None, **kwargs) -> str:
    """ç›´æ¥è°ƒç”¨Ollama APIçš„è§†è§‰æ¨¡å‹å‡½æ•°"""
    import requests
    import json
    
    try:
        # æ„å»ºå®Œæ•´çš„æç¤ºè¯
        if system_prompt:
            full_prompt = f"{system_prompt}\n\n{prompt}"
        else:
            full_prompt = prompt
        
        # æ„å»ºè¯·æ±‚æ•°æ® - ç›´æ¥ä½¿ç”¨Ollama APIæ ¼å¼
        api_url = f"{OLLAMA_HOST}/api/generate"
        data = {
            "model": VISION_MODEL,
            "prompt": full_prompt,
            "stream": False,
            "options": {"num_ctx": 4096}
        }
        
        # å¦‚æœæœ‰å›¾åƒæ•°æ®ï¼Œæ·»åŠ åˆ°è¯·æ±‚ä¸­
        if image_data:
            data["images"] = [image_data]
        
        # å‘é€è¯·æ±‚ï¼Œå¢åŠ é‡è¯•æœºåˆ¶
        max_retries = 3
        for attempt in range(max_retries):
            try:
                # å¢åŠ è¶…æ—¶æ—¶é—´ï¼Œå¤„ç†å¤§å›¾ç‰‡
                response = requests.post(api_url, json=data, timeout=TIMEOUT*2)
                response.raise_for_status()
                break
            except requests.exceptions.RequestException as e:
                if attempt == max_retries - 1:
                    raise e
                logger.warning(f"è§†è§‰æ¨¡å‹APIè°ƒç”¨å°è¯• {attempt + 1} å¤±è´¥ï¼Œ5ç§’åé‡è¯•...")
                await asyncio.sleep(5)
        
        # è§£æå“åº”
        try:
            result = response.json()
            description = result.get("response", "").strip()
            
            if description:
                return description
            else:
                return "æ— æ³•ç”Ÿæˆå›¾ç‰‡æè¿°"
        except json.JSONDecodeError as je:
            logger.error(f"JSONè§£æé”™è¯¯: {je}")
            # å°è¯•ä»å“åº”æ–‡æœ¬ä¸­æå–å†…å®¹
            try:
                response_text = response.text.strip()
                logger.debug(f"åŸå§‹å“åº”å†…å®¹: {response_text[:500]}...")
                
                # å¦‚æœå“åº”åŒ…å«æœ‰æ•ˆå†…å®¹ä½†ä¸æ˜¯JSONæ ¼å¼ï¼Œç›´æ¥ä½¿ç”¨
                if response_text and len(response_text) > 10 and not response_text.startswith("ERROR"):
                    # å°è¯•æå–å¯èƒ½çš„JSONéƒ¨åˆ†
                    if "{" in response_text and "}" in response_text:
                        # æŸ¥æ‰¾æœ€åä¸€ä¸ªå®Œæ•´çš„JSONå¯¹è±¡
                        json_start = response_text.rfind("{")
                        json_end = response_text.rfind("}") + 1
                        if json_start >= 0 and json_end > json_start:
                            json_part = response_text[json_start:json_end]
                            
                            # å°è¯•ä¿®å¤å¸¸è§çš„JSONæ ¼å¼é—®é¢˜
                            json_part = re.sub(r',\s*([}\]])', r'\1', json_part)  # ç§»é™¤æœ«å°¾å¤šä½™çš„é€—å·
                            json_part = re.sub(r'([^"\\])"([^":])', r'\1\\"\2', json_part)  # è½¬ä¹‰å¼•å·
                            
                            try:
                                fixed_result = json.loads(json_part)
                                description = fixed_result.get("response", "").strip()
                                if description:
                                    logger.info("æˆåŠŸä¿®å¤å¹¶è§£æJSONå“åº”")
                                    return description
                            except json.JSONDecodeError:
                                logger.warning("JSONä¿®å¤å°è¯•å¤±è´¥")
                    
                    # å¦‚æœä¸æ˜¯JSONï¼Œå¯èƒ½æ˜¯çº¯æ–‡æœ¬å“åº”ï¼Œç›´æ¥è¿”å›
                    if not response_text.startswith("{") and not response_text.startswith("["):
                        logger.info("ä½¿ç”¨çº¯æ–‡æœ¬å“åº”ä½œä¸ºå›¾ç‰‡æè¿°")
                        return response_text
                    
                return "æ— æ³•ç”Ÿæˆå›¾ç‰‡æè¿°"
                    
            except Exception as text_error:
                logger.error(f"å¤„ç†å“åº”æ–‡æœ¬å¤±è´¥: {text_error}")
                return "å›¾ç‰‡åˆ†æå¤±è´¥: å“åº”å¤„ç†é”™è¯¯"
                
    except Exception as e:
        logger.error(f"ç›´æ¥è§†è§‰æ¨¡å‹APIè°ƒç”¨å¤±è´¥: {e}")
        return f"å›¾ç‰‡åˆ†æå¤±è´¥: {str(e)}"


async def ollama_vision_complete(prompt: str, system_prompt: str = None, history_messages: list = None, image_data: str = None, **kwargs) -> str:
    """è§†è§‰æ¨¡å‹è°ƒç”¨å‡½æ•° - åŒ…è£…ç›´æ¥APIè°ƒç”¨ä»¥ä¿æŒæ¥å£å…¼å®¹æ€§"""
    return await ollama_vision_complete_direct(
        prompt=prompt,
        system_prompt=system_prompt,
        image_data=image_data,
        **kwargs
    )


class OllamaImageDescriber:
    """ä½¿ç”¨Ollama APIæè¿°å›¾ç‰‡å†…å®¹çš„ç±»"""
    
    def __init__(self, base_url: str = OLLAMA_HOST, model: str = VISION_MODEL):
        self.base_url = base_url
        self.model = model
        self.api_url = f"{base_url}/api/generate"
    
    def encode_image_to_base64(self, image_path: str) -> str:
        """å°†å›¾ç‰‡ç¼–ç ä¸ºbase64æ ¼å¼"""
        try:
            with open(image_path, "rb") as image_file:
                encoded_string = base64.b64encode(image_file.read()).decode('utf-8')
                return encoded_string
        except Exception as e:
            logger.error(f"Failed to encode image {image_path}: {e}")
            return None
    
    async def describe_image(self, image_path: str, prompt: str = None) -> str:
        """ä½¿ç”¨Ollamaæ¨¡å‹æè¿°å›¾ç‰‡å†…å®¹""" 
        
        # ä½¿ç”¨æœ¬åœ°æç¤ºè¯æ–‡ä»¶ä¸­çš„å›¾è¡¨åˆ†ææç¤ºè¯
        if prompt is None:
            prompt = local_prompt.PROMPTS["financial_chart_analysis"]
        
        try:
            # ç¼–ç å›¾ç‰‡
            base64_image = self.encode_image_to_base64(image_path)
            if not base64_image:
                return f"[å›¾ç‰‡åŠ è½½å¤±è´¥: {os.path.basename(image_path)}]"
            
            # æ„å»ºè¯·æ±‚æ•°æ®
            data = {
                "model": self.model,
                "prompt": prompt,
                "images": [base64_image],
                "stream": False,
                "options": {"num_ctx": 4096}
            }
            
            # å‘é€è¯·æ±‚ï¼Œå¢åŠ é‡è¯•æœºåˆ¶
            max_retries = 3
            for attempt in range(max_retries):
                try:
                    import requests
                    # å¢åŠ è¶…æ—¶æ—¶é—´ä»¥é€‚åº”å¤§å›¾ç‰‡
                    response = requests.post(self.api_url, json=data, timeout=TIMEOUT*2)
                    response.raise_for_status()
                    break
                except requests.exceptions.RequestException as e:
                    if attempt == max_retries - 1:
                        raise e
                    logger.warning(f"Attempt {attempt + 1} failed, retrying in 5 seconds...")
                    await asyncio.sleep(5)
            
            # è§£æå“åº” - å¢å¼ºé”™è¯¯å¤„ç†
            try:
                import json
                result = response.json()
                description = result.get("response", "").strip()
                
                if description:
                    logger.info(f"Successfully described image: {os.path.basename(image_path)}")
                    return description
                else:
                    return f"[æ— æ³•ç”Ÿæˆå›¾ç‰‡æè¿°: {os.path.basename(image_path)}]"
            except json.JSONDecodeError as je:
                logger.error(f"å›¾ç‰‡æè¿°JSONè§£æé”™è¯¯: {je}")
                # å°è¯•ä»å“åº”æ–‡æœ¬ä¸­æå–å†…å®¹
                try:
                    response_text = response.text.strip()
                    logger.debug(f"å›¾ç‰‡æè¿°åŸå§‹å“åº”: {response_text[:300]}...")
                    
                    # å¦‚æœå“åº”åŒ…å«æœ‰æ•ˆå†…å®¹ä½†ä¸æ˜¯JSONæ ¼å¼ï¼Œç›´æ¥ä½¿ç”¨
                    if response_text and len(response_text) > 10 and not response_text.startswith("ERROR"):
                        # å°è¯•æå–å¯èƒ½çš„JSONéƒ¨åˆ†
                        if "{" in response_text and "}" in response_text:
                            # æŸ¥æ‰¾æœ€åä¸€ä¸ªå®Œæ•´çš„JSONå¯¹è±¡
                            json_start = response_text.rfind("{")
                            json_end = response_text.rfind("}") + 1
                            if json_start >= 0 and json_end > json_start:
                                json_part = response_text[json_start:json_end]
                                
                                # å°è¯•ä¿®å¤å¸¸è§çš„JSONæ ¼å¼é—®é¢˜
                                json_part = re.sub(r',\s*([}\]])', r'\1', json_part)  # ç§»é™¤æœ«å°¾å¤šä½™çš„é€—å·
                                json_part = re.sub(r'([^"\\])"([^":])', r'\1\\"\2', json_part)  # è½¬ä¹‰å¼•å·
                                
                                try:
                                    fixed_result = json.loads(json_part)
                                    description = fixed_result.get("response", "").strip()
                                    if description:
                                        logger.info(f"ä¿®å¤JSONåæˆåŠŸæè¿°å›¾ç‰‡: {os.path.basename(image_path)}")
                                        return description
                                except json.JSONDecodeError:
                                    logger.warning("å›¾ç‰‡æè¿°JSONä¿®å¤å°è¯•å¤±è´¥")
                        
                        # å¦‚æœä¸æ˜¯JSONï¼Œå¯èƒ½æ˜¯çº¯æ–‡æœ¬å“åº”ï¼Œç›´æ¥è¿”å›
                        if not response_text.startswith("{") and not response_text.startswith("["):
                            logger.info(f"ä½¿ç”¨çº¯æ–‡æœ¬ä½œä¸ºå›¾ç‰‡æè¿°: {os.path.basename(image_path)}")
                            return response_text
                        
                    return f"[æ— æ³•ç”Ÿæˆå›¾ç‰‡æè¿°: {os.path.basename(image_path)}]"
                        
                except Exception as text_error:
                    logger.error(f"å¤„ç†å›¾ç‰‡æè¿°å“åº”æ–‡æœ¬å¤±è´¥: {text_error}")
                    return f"[å›¾ç‰‡æè¿°å¤±è´¥(å“åº”å¤„ç†é”™è¯¯): {os.path.basename(image_path)}]"
                
        except Exception as e:
            logger.error(f"Failed to describe image {image_path}: {e}")
            return f"[å›¾ç‰‡æè¿°å¤±è´¥: {os.path.basename(image_path)}]"
    
    def describe_image_sync(self, image_path: str, prompt: str = None) -> str:
        """ä½¿ç”¨Ollamaæ¨¡å‹æè¿°å›¾ç‰‡å†…å®¹"""
        
        # ä½¿ç”¨é‡‘èå›¾è¡¨åˆ†æä¸“ç”¨æç¤ºè¯
        if prompt is None:
            prompt = """
ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„é‡‘èå›¾è¡¨åˆ†æå¸ˆã€‚è¯·ä»”ç»†è§‚å¯Ÿè¿™å¼ å›¾ç‰‡ï¼Œç”¨ä¸­æ–‡è¯¦ç»†æè¿°å…¶å†…å®¹ã€‚è¯·ä¸¥æ ¼æŒ‰ç…§å®¢è§‚äº‹å®æè¿°ï¼Œä¸è¦è¿›è¡Œä¸»è§‚è§£è¯»ã€‚

è¯·æŒ‰ä»¥ä¸‹æ ¼å¼æè¿°ï¼š

ã€å›¾è¡¨ç±»å‹ã€‘ï¼šæ˜ç¡®è¯´æ˜è¿™æ˜¯ä»€ä¹ˆç±»å‹çš„å›¾è¡¨ï¼ˆå¦‚ï¼šæŠ˜çº¿å›¾ã€æŸ±çŠ¶å›¾ã€Kçº¿å›¾ã€æ•£ç‚¹å›¾ã€è¡¨æ ¼ç­‰ï¼‰

ã€æ ‡é¢˜ä¿¡æ¯ã€‘ï¼šå¦‚æœæœ‰æ ‡é¢˜ï¼Œè¯·å®Œæ•´å‡†ç¡®åœ°æ‘˜å½•

ã€åæ ‡è½´ã€‘ï¼š
- Xè½´ï¼šæ ‡ç­¾å†…å®¹ã€æ•°å€¼èŒƒå›´ã€åˆ»åº¦é—´éš”
- Yè½´ï¼šæ ‡ç­¾å†…å®¹ã€æ•°å€¼èŒƒå›´ã€åˆ»åº¦é—´éš”
- å¦‚æœ‰å³Yè½´ï¼Œä¹Ÿè¦æè¿°

ã€æ•°æ®ç³»åˆ—ã€‘ï¼šæè¿°å›¾ä¸­æœ‰å‡ æ¡çº¿/æŸ±å­/æ•°æ®ç³»åˆ—ï¼Œæ¯ä¸ªç³»åˆ—çš„é¢œè‰²ã€æ ·å¼ã€å›¾ä¾‹åç§°

ã€å…³é”®æ•°å€¼ã€‘ï¼šæŒ‡å‡ºå›¾ä¸­çš„æœ€é«˜ç‚¹ã€æœ€ä½ç‚¹ã€é‡è¦è½¬æŠ˜ç‚¹çš„å…·ä½“æ•°å€¼å’Œä½ç½®

ã€å›¾ä¾‹å’Œæ ‡æ³¨ã€‘ï¼šæè¿°å›¾ä¾‹ä½ç½®ã€å†…å®¹ï¼Œä»¥åŠå›¾ä¸­çš„æ–‡å­—æ ‡æ³¨ã€ç®­å¤´ç­‰

ã€å…¶ä»–å…ƒç´ ã€‘ï¼šç½‘æ ¼çº¿ã€èƒŒæ™¯è‰²ã€æ°´å°ç­‰è§†è§‰å…ƒç´ 

è¯·ç”¨å…·ä½“çš„æ•°å­—å’Œæè¿°è¯å¡«å……ä»¥ä¸Šå†…å®¹ï¼Œä¸è¦ä½¿ç”¨å ä½ç¬¦æˆ–æ–¹æ‹¬å·ã€‚
            """
        
        try:
            # ç¼–ç å›¾ç‰‡
            base64_image = self.encode_image_to_base64(image_path)
            if not base64_image:
                return f"[å›¾ç‰‡åŠ è½½å¤±è´¥: {os.path.basename(image_path)}]"
            
            # æ„å»ºè¯·æ±‚æ•°æ®
            data = {
                "model": self.model,
                "prompt": prompt,
                "images": [base64_image],
                "stream": False,
                "options": {"num_ctx": 4096}
            }
            
            # å‘é€è¯·æ±‚ï¼Œå¢åŠ é‡è¯•æœºåˆ¶
            max_retries = 3
            for attempt in range(max_retries):
                try:
                    import requests
                    response = requests.post(self.api_url, json=data, timeout=120)
                    response.raise_for_status()
                    break
                except requests.exceptions.RequestException as e:
                    if attempt == max_retries - 1:
                        raise e
                    logger.warning(f"Attempt {attempt + 1} failed, retrying in 5 seconds...")
                    time.sleep(5)
            
            # è§£æå“åº”
            try:
                result = response.json()
                description = result.get("response", "").strip()
                
                if description:
                    logger.info(f"Successfully described image: {os.path.basename(image_path)}")
                    return description
                else:
                    return f"[æ— æ³•ç”Ÿæˆå›¾ç‰‡æè¿°: {os.path.basename(image_path)}]"
            except json.JSONDecodeError as je:
                logger.error(f"å›¾ç‰‡æè¿°JSONè§£æé”™è¯¯: {je}")
                # å°è¯•ç›´æ¥ä½¿ç”¨å“åº”æ–‡æœ¬
                try:
                    response_text = response.text.strip()
                    if response_text and len(response_text) > 10 and not response_text.startswith("ERROR"):
                        # å¦‚æœä¸æ˜¯JSONï¼Œå¯èƒ½æ˜¯çº¯æ–‡æœ¬å“åº”ï¼Œç›´æ¥è¿”å›
                        if not response_text.startswith("{") and not response_text.startswith("["):
                            logger.info(f"ä½¿ç”¨çº¯æ–‡æœ¬ä½œä¸ºå›¾ç‰‡æè¿°: {os.path.basename(image_path)}")
                            return response_text
                    return f"[æ— æ³•ç”Ÿæˆå›¾ç‰‡æè¿°: {os.path.basename(image_path)}]"
                except Exception as text_error:
                    logger.error(f"å¤„ç†å›¾ç‰‡æè¿°å“åº”æ–‡æœ¬å¤±è´¥: {text_error}")
                    return f"[å›¾ç‰‡æè¿°å¤±è´¥(å“åº”å¤„ç†é”™è¯¯): {os.path.basename(image_path)}]"
                
        except Exception as e:
            logger.error(f"Failed to describe image {image_path}: {e}")
            return f"[å›¾ç‰‡æè¿°å¤±è´¥: {os.path.basename(image_path)}]"


def get_ollama_vision_model_func():
    """è·å–å…¼å®¹ modalprocessors çš„è§†è§‰æ¨¡å‹å‡½æ•°"""
    async def vision_model_wrapper(prompt, system_prompt=None, history_messages=None, image_data=None, **kwargs):
        # ç¡®ä¿ä¸ä¼šæœ‰é‡å¤å‚æ•°
        vision_kwargs = {
            "host": OLLAMA_HOST,
            "options": {"num_ctx": 4096},
            "timeout": TIMEOUT,
        }
        
        # åªæ·»åŠ ä¸å†²çªçš„kwargsï¼Œå¹¶æ’é™¤å¯èƒ½å¯¼è‡´å†²çªçš„å‚æ•°
        for key, value in kwargs.items():
            if key not in vision_kwargs and key not in ["model"]:
                vision_kwargs[key] = value
        
        # ç¡®ä¿ä¸åŒ…å«modelå‚æ•°ï¼Œé¿å…ä¸å†…éƒ¨é€»è¾‘å†²çª
        vision_kwargs.pop("model", None)
        
        return await ollama_vision_complete(
            prompt=prompt,
            system_prompt=system_prompt,
            history_messages=history_messages or [],
            image_data=image_data,
            **vision_kwargs
        )
    
    return vision_model_wrapper


async def ollama_embedding_func(texts: List[str]) -> List[List[float]]:
    """åµŒå…¥æ¨¡å‹è°ƒç”¨å‡½æ•°"""
    try:
        return await ollama_embed(
            texts=texts,
            embed_model=EMBEDDING_MODEL,
            host=OLLAMA_HOST,
        )
    except Exception as e:
        logger.error(f"åµŒå…¥æ¨¡å‹è°ƒç”¨å¤±è´¥: {e}")
        # è¿”å›é»˜è®¤ç»´åº¦çš„é›¶å‘é‡
        return [[0.0] * 1024 for _ in texts]


async def simple_ollama_model_func(prompt, system_prompt=None, history_messages=None, **kwargs):
    """æ ¹æ®ä»»åŠ¡ç±»å‹åŠ¨æ€é€‰æ‹©Ollamaæ¨¡å‹çš„è°ƒç”¨å‡½æ•°"""
    # æ”¯æŒçš„æ¨¡å‹é…ç½®
    extraction_model = os.getenv("EXTRACTION_MODEL", "qwen3:1.7b-q4_K_M")
    answer_model = os.getenv("ANSWER_MODEL", "qwen3:8b-q4_K_M")
    # å…è®¸å¤–éƒ¨ç›´æ¥æŒ‡å®šæ¨¡å‹
    model_name = kwargs.pop("model", None)
    # ä»»åŠ¡ç±»å‹ä¼˜å…ˆçº§ï¼štask/llm_task > model > é»˜è®¤
    task = kwargs.pop("task", None) or kwargs.pop("llm_task", None)
    if not model_name:
        if task in ("extract", "extraction", "entity_extraction", "relation_extraction"):
            model_name = extraction_model
        elif task in ("answer", "qa", "question_answering", "generate", "completion"):
            model_name = answer_model
        else:
            # é»˜è®¤ç”¨é—®ç­”æ¨¡å‹
            model_name = answer_model
    # å½»åº•æ¸…ç†å‚æ•°ä»¥é¿å…å†²çª
    safe_kwargs = {}
    safe_internal_params = ["hashing_kv", "embedding_func", "semaphore"]
    for key in safe_internal_params:
        if key in kwargs:
            safe_kwargs[key] = kwargs[key]
    safe_kwargs.update({
        "host": OLLAMA_HOST,
        "options": {"num_ctx": 8192},
        "timeout": TIMEOUT,
    })
    safe_kwargs.pop("model", None)
    try:
        api_url = f"{OLLAMA_HOST}/api/generate"
        if system_prompt:
            full_prompt = f"{system_prompt}\n\n{prompt}"
        else:
            full_prompt = prompt
        data = {
            "model": model_name,
            "prompt": full_prompt,
            "stream": False,
            "options": {"num_ctx": 8192}
        }
        response = requests.post(api_url, json=data, timeout=TIMEOUT)
        response.raise_for_status()
        result = response.json()
        return result.get("response", "").strip()
    except Exception as e:
        logger.error(f"ç›´æ¥APIè°ƒç”¨å¤±è´¥ï¼Œå›é€€åˆ°LightRAGè°ƒç”¨: {e}")
        return await ollama_model_complete(
            prompt=prompt,
            system_prompt=system_prompt,
            history_messages=history_messages or [],
            **safe_kwargs
        )


async def modal_ollama_model_func(prompt, system_prompt=None, history_messages=None, **kwargs):
    """ä¸ºæ¨¡æ€å¤„ç†å™¨å®šåˆ¶çš„ollamaæ¨¡å‹å‡½æ•° - ä½¿ç”¨ç¯å¢ƒå˜é‡é…ç½®çš„æ¨¡å‹"""
    
    # ä»ç¯å¢ƒå˜é‡è·å–æ¨¡å‹é…ç½®ï¼Œé»˜è®¤ä½¿ç”¨å›ç­”æ¨¡å‹
    model_name = os.getenv("FORCE_MODEL", ANSWER_MODEL)
    logger.debug(f"æ¨¡æ€å¤„ç†å™¨ä½¿ç”¨æ¨¡å‹: {model_name}")
    
    # ä½¿ç”¨ç›´æ¥APIè°ƒç”¨é¿å…LightRAGå‚æ•°å†²çª
    try:
        api_url = f"{OLLAMA_HOST}/api/generate"
        
        # æ„å»ºå®Œæ•´çš„æç¤ºè¯
        if system_prompt:
            full_prompt = f"{system_prompt}\n\n{prompt}"
        else:
            full_prompt = prompt
        
        data = {
            "model": model_name,
            "prompt": full_prompt,
            "stream": False,
            "options": {"num_ctx": 4096}  # æ¨¡æ€å¤„ç†ä½¿ç”¨ç¨å°çš„ä¸Šä¸‹æ–‡
        }
        
        response = requests.post(api_url, json=data, timeout=TIMEOUT)
        response.raise_for_status()
        result = response.json()
        
        return result.get("response", "").strip()
        
    except Exception as e:
        logger.error(f"æ¨¡æ€å¤„ç†å™¨ç›´æ¥APIè°ƒç”¨å¤±è´¥ï¼Œå›é€€åˆ°LightRAGè°ƒç”¨: {e}")
        
        # å½»åº•æ¸…ç†å‚æ•°ä»¥é¿å…å†²çª
        safe_kwargs = {}
        
        # åªä¿ç•™å®‰å…¨çš„å†…éƒ¨å‚æ•°
        safe_internal_params = ["hashing_kv", "embedding_func", "semaphore"]
        for key in safe_internal_params:
            if key in kwargs:
                safe_kwargs[key] = kwargs[key]
        
        # æ·»åŠ è‡ªå®šä¹‰å‚æ•°ï¼Œä½†æ’é™¤å¯èƒ½å¯¼è‡´å†²çªçš„å‚æ•°
        safe_kwargs.update({
            "host": OLLAMA_HOST,
            "options": {"num_ctx": 4096},  # æ¨¡æ€å¤„ç†ä½¿ç”¨ç¨å°çš„ä¸Šä¸‹æ–‡
            "timeout": TIMEOUT,
        })
        
        # ç¡®ä¿ä¸åŒ…å«modelå‚æ•°ï¼Œé¿å…ä¸LightRAGå†…éƒ¨é€»è¾‘å†²çª
        safe_kwargs.pop("model", None)
        
        # é€šè¿‡ä¿®æ”¹hashing_kvä¸­çš„é…ç½®æ¥åŠ¨æ€è®¾ç½®æ¨¡å‹å
        if "hashing_kv" in safe_kwargs and hasattr(safe_kwargs["hashing_kv"], "global_config"):
            # ä¸´æ—¶ä¿å­˜åŸå§‹æ¨¡å‹å
            original_model = safe_kwargs["hashing_kv"].global_config.get("llm_model_name", ANSWER_MODEL)
            # è®¾ç½®æ–°çš„æ¨¡å‹å
            safe_kwargs["hashing_kv"].global_config["llm_model_name"] = model_name
            logger.debug(f"æ¨¡æ€å¤„ç†å™¨é€šè¿‡hashing_kvè®¾ç½®æ¨¡å‹: {original_model} -> {model_name}")
        
        return await ollama_model_complete(
            prompt=prompt,
            system_prompt=system_prompt,
            history_messages=history_messages or [],
            **safe_kwargs
        )


async def ollama_rerank_func(query: str, documents: List[Dict], top_n: int = 10, **kwargs) -> List[Dict]:
    """æœ¬åœ°é‡æ’åºå‡½æ•° - ä½¿ç”¨é…ç½®çš„é‡æ’åºæ¨¡å‹"""
    try:
        # æ„å»ºé‡æ’åºçš„è¾“å…¥æ ¼å¼
        doc_texts = [doc.get("content", str(doc)) for doc in documents]
        
        # ä½¿ç”¨é…ç½®çš„é‡æ’åºæ¨¡å‹è¿›è¡Œè¯„åˆ†
        scores = []
        for doc_text in doc_texts:
            # ä½¿ç”¨æœ¬åœ°æç¤ºè¯æ–‡ä»¶ä¸­çš„é‡æ’åºè¯„åˆ†æç¤ºè¯
            score_prompt = local_prompt.PROMPTS["financial_rerank_score"].format(
                query=query,
                doc_text=doc_text
            )
            
            try:
                # ä½¿ç”¨ç›´æ¥APIè°ƒç”¨
                api_url = f"{OLLAMA_HOST}/api/generate"
                data = {
                    "model": RERANK_MODEL,  # ä½¿ç”¨é…ç½®çš„é‡æ’åºæ¨¡å‹
                    "prompt": score_prompt,
                    "system": "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„é‡‘èæ–‡æ¡£ç›¸å…³æ€§è¯„ä¼°ä¸“å®¶ï¼Œæ“…é•¿åˆ¤æ–­æ–‡æ¡£ä¸ç”¨æˆ·é‡‘èæŸ¥è¯¢çš„ç›¸å…³ç¨‹åº¦ã€‚è¯·å®¢è§‚å…¬æ­£åœ°è¯„ä¼°ï¼Œåªç»™å‡º0-1ä¹‹é—´çš„æ•°å€¼è¯„åˆ†ï¼Œä¸è¦æ·»åŠ ä»»ä½•å…¶ä»–æ–‡å­—ã€‚",
                    "stream": False,
                    "options": {"num_ctx": 2048}
                }
                
                import requests
                response = requests.post(api_url, json=data, timeout=60)
                response.raise_for_status()
                result = response.json()
                score_response = result.get("response", "").strip()
                
                # å°è¯•æå–æ•°å­—åˆ†æ•°
                import re
                score_match = re.search(r'(\d+\.?\d*)', score_response)
                score = float(score_match.group(1)) if score_match else 0.5
                score = max(0.0, min(1.0, score))  # é™åˆ¶åœ¨0-1èŒƒå›´å†…
            except:
                score = 0.5  # é»˜è®¤åˆ†æ•°
                
            scores.append(score)
        
        # æ ¹æ®åˆ†æ•°æ’åºå¹¶è¿”å›top_n
        scored_docs = list(zip(documents, scores))
        scored_docs.sort(key=lambda x: x[1], reverse=True)
        
        result = []
        for doc, score in scored_docs[:top_n]:
            result_doc = doc.copy()
            result_doc["rerank_score"] = score
            result.append(result_doc)
            
        return result
        
    except Exception as e:
        logger.error(f"é‡æ’åºå¤±è´¥: {e}")
        return documents[:top_n]


class FinancialReportProcessor:
    """é‡‘èæŠ¥å‘Šå¤„ç†å™¨ä¸»ç±»"""
    
    def __init__(self, working_dir: str = WORKING_DIR):
        self.working_dir = working_dir
        
        # å…ˆåº”ç”¨è¡¥ä¸å‡½æ•°ï¼Œé¿å…åç»­åˆå§‹åŒ–å‡ºé”™
        self._patch_raganything_mineru_check()
        
        # ç¡®ä¿æ‰€æœ‰éœ€è¦çš„ç›®å½•éƒ½å­˜åœ¨
        os.makedirs(working_dir, exist_ok=True)
        os.makedirs(OUTPUT_DIR, exist_ok=True)
        os.makedirs(os.path.join(self.working_dir, "financial_reports"), exist_ok=True)
        
        # ç¡®ä¿æ•°æ®ç›®å½•å­˜åœ¨
        reports_dir = os.path.join(self.working_dir, "financial_reports")
        os.makedirs(reports_dir, exist_ok=True)
        
        # åˆ›å»ºå¿…è¦çš„ç©ºæ–‡ä»¶ï¼Œç¡®ä¿æ–‡ä»¶å­˜åœ¨
        for file_name in ["kv_store_doc_status.json", "kv_store_full_docs.json", "kv_store_text_chunks.json"]:
            file_path = os.path.join(reports_dir, file_name)
            if not os.path.exists(file_path):
                with open(file_path, 'w') as f:
                    f.write("{}")
        
        self.rag = None
        self.modal_processors = {}
        
        # åˆå§‹åŒ–å›¾ç‰‡æè¿°å™¨
        self.image_describer = OllamaImageDescriber(OLLAMA_HOST, VISION_MODEL)
    
    def _patch_raganything_mineru_check(self):
        """ä¿®å¤ RAGAnything å¯¹ MinerU çš„æ£€æµ‹ï¼Œå¼ºåˆ¶è®¾ç½®ä¸ºå·²å®‰è£…çŠ¶æ€"""
        if HAS_RAGANYTHING:
            try:
                # å¯¼å…¥éœ€è¦ä¿®æ”¹çš„æ¨¡å—
                import raganything.utils
                import raganything
                
                # åŠ¨æ€ä¿®æ”¹ RAGAnything çš„æ£€æµ‹å‡½æ•°ï¼Œä½¿å…¶å§‹ç»ˆè¿”å› True
                original_check = raganything.utils.check_mineru_installation
                raganything.utils.check_mineru_installation = lambda: True
                
                # å¦‚æœæœ‰__mineru_installed__å±æ€§ï¼Œä¹Ÿè®¾ç½®ä¸ºTrue
                if hasattr(raganything, "__mineru_installed__"):
                    raganything.__mineru_installed__ = True
                    
                # è¡¥ä¸å…¶ä»–å¯èƒ½ä½¿ç”¨è¯¥æ£€æŸ¥çš„åœ°æ–¹
                try:
                    # å°è¯•å¯¼å…¥å¤„ç†æ–‡æ¡£çš„ç±»
                    import importlib
                    document_processor = importlib.import_module("raganything.docprocessor")
                    if hasattr(document_processor, "DocumentProcessor"):
                        processor_class = getattr(document_processor, "DocumentProcessor")
                        if hasattr(processor_class, "_check_mineru"):
                            processor_class._check_mineru = lambda self: True
                except (ImportError, AttributeError):
                    pass
                
                # ç¡®ä¿é…ç½®ç±»ä¹ŸçŸ¥é“MinerUå·²å®‰è£…
                try:
                    from raganything import RAGAnythingConfig
                    original_init = RAGAnythingConfig.__init__
                    
                    def patched_init(self, *args, **kwargs):
                        result = original_init(self, *args, **kwargs)
                        self.mineru_installed = True
                        return result
                    
                    RAGAnythingConfig.__init__ = patched_init
                except Exception:
                    pass
                
                logger.info("âœ… å·²ä¿®å¤ RAGAnything çš„ MinerU æ£€æµ‹åŠŸèƒ½")
            except Exception as e:
                logger.warning(f"æ— æ³•ä¿®å¤ RAGAnything çš„ MinerU æ£€æµ‹: {e}")
    
    async def initialize(self):
        """åˆå§‹åŒ–å¤„ç†å™¨"""
        
        # æ¸…ç†æ—§æ•°æ®æ–‡ä»¶
        await self._clean_old_data()
        
        # åˆå§‹åŒ–RAGç³»ç»Ÿ
        await self._initialize_rag()
        
        # ä¸å†éœ€è¦æ¨¡æ€å¤„ç†å™¨ï¼Œå›¾ç‰‡å¤„ç†ç›´æ¥é€šè¿‡ Ollama å®Œæˆ
        logger.info("âœ… å¤„ç†å™¨åˆå§‹åŒ–å®Œæˆ - ä½¿ç”¨ç›´æ¥Ollamaå›¾ç‰‡å¤„ç†æ¨¡å¼")
    
    async def _clean_old_data(self):
        """æ¸…ç†æ—§çš„æ•°æ®æ–‡ä»¶"""
        # ä½¿ç”¨æ­£ç¡®çš„å·¥ä½œç›®å½•è·¯å¾„
        reports_dir = os.path.join(self.working_dir, "financial_reports")
        
        files_to_delete = [
            "graph_chunk_entity_relation.graphml",
            "kv_store_doc_status.json", 
            "kv_store_full_docs.json",
            "kv_store_text_chunks.json",
            "vdb_chunks.json",
            "vdb_entities.json",
            "vdb_relationships.json",
        ]
        
        for file in files_to_delete:
            file_path = os.path.join(reports_dir, file)
            if os.path.exists(file_path):
                os.remove(file_path)
                logger.info(f"åˆ é™¤æ—§æ–‡ä»¶: {file_path}")
    
    async def save_content_to_file(self, content: Union[str, dict, list], filename: str, directory: str = OUTPUT_DIR) -> str:
        """ä¿å­˜å†…å®¹åˆ°æ–‡ä»¶ï¼Œç¡®ä¿ç›®å½•å­˜åœ¨"""
        os.makedirs(directory, exist_ok=True)
        file_path = os.path.join(directory, filename)
        
        try:
            if isinstance(content, (dict, list)):
                with open(file_path, 'w', encoding='utf-8') as f:
                    json.dump(content, f, ensure_ascii=False, indent=2)
            else:
                with open(file_path, 'w', encoding='utf-8') as f:
                    f.write(str(content))
            logger.info(f"å†…å®¹å·²ä¿å­˜åˆ°: {file_path}")
            return file_path
        except Exception as e:
            logger.error(f"ä¿å­˜å†…å®¹åˆ°æ–‡ä»¶å¤±è´¥: {e}")
            return ""
    
    async def _initialize_rag(self, cleanup_old_data: bool = True):
        """åˆå§‹åŒ–RAGç³»ç»Ÿ - æ”¯æŒæ˜¯å¦æ¸…ç†æ—§æ•°æ®çš„é€‰é¡¹"""
        reports_dir = os.path.join(self.working_dir, "knowledge_base")
        os.makedirs(reports_dir, exist_ok=True)
        
        logger.info(f"ä½¿ç”¨çŸ¥è¯†åº“ç›®å½•: {reports_dir}")
        
        # å¦‚æœéœ€è¦æ¸…ç†ï¼Œåˆ™åˆ é™¤æ—§æ•°æ®
        if cleanup_old_data:
            await self._clean_old_data()
        
        # ç¡®ä¿å¿…è¦çš„KVå­˜å‚¨æ–‡ä»¶å­˜åœ¨
        for filename in ["kv_store_doc_status.json", "kv_store_full_docs.json", "kv_store_text_chunks.json"]:
            file_path = os.path.join(reports_dir, filename)
            if not os.path.exists(file_path):
                with open(file_path, 'w') as f:
                    f.write("{}")
                logger.info(f"åˆ›å»ºç©ºKVå­˜å‚¨æ–‡ä»¶: {file_path}")
        
        # æµ‹è¯•åµŒå…¥åŠŸèƒ½å¹¶è·å–ç»´åº¦
        test_embedding = await ollama_embedding_func(["æµ‹è¯•æ–‡æœ¬"])
        embedding_dim = len(test_embedding[0])
        logger.info(f"æ£€æµ‹åˆ°åµŒå…¥ç»´åº¦: {embedding_dim}")
        
        # è·å–æœ¬åœ°æç¤ºè¯å¹¶åº”ç”¨
        get_local_prompts()
        
        # åˆ›å»ºLightRAGå®ä¾‹ï¼Œä½¿ç”¨ç®€åŒ–çš„ç›®å½•ç»“æ„
        logger.info(f"LightRAGå°†ä½¿ç”¨å·¥ä½œç›®å½•: {reports_dir}")
        # åŒ…è£…llm_model_funcï¼Œè‡ªåŠ¨è¯†åˆ«ä»»åŠ¡ç±»å‹
        async def llm_model_func_with_task(prompt, system_prompt=None, history_messages=None, **kwargs):
            # ä¼˜å…ˆçº§1ï¼šå¦‚æœæ˜ç¡®æŒ‡å®šäº†taskæˆ–modelï¼Œç›´æ¥ç”¨
            if "task" in kwargs and kwargs["task"]:
                return await simple_ollama_model_func(prompt, system_prompt, history_messages, **kwargs)
            if "model" in kwargs and kwargs["model"]:
                return await simple_ollama_model_func(prompt, system_prompt, history_messages, **kwargs)
            # ä¼˜å…ˆçº§2ï¼šå¦‚æœsystem_promptæˆ–promptä¸­åŒ…å«æ¨¡å‹åå…³é”®è¯ï¼Œå¼ºåˆ¶åˆ‡æ¢
            check_text = (system_prompt or "") + "\n" + (prompt or "")
            extraction_model = os.getenv("EXTRACTION_MODEL", "qwen3:1.7b-q4_K_M")
            answer_model = os.getenv("ANSWER_MODEL", "qwen3:8b-q4_K_M")
            extraction_keywords = ["å®ä½“æå–", "å…³ç³»æå–", "entity extraction", "relation extraction", "ä¿¡æ¯æŠ½å–", "extraction task", "extract entities", "extract relations"]
            answer_keywords = ["é—®ç­”", "å›ç­”", "answer", "qa", "question", "generate", "completion"]
            # å¦‚æœæ–‡æœ¬ä¸­å‡ºç°äº†æå–æ¨¡å‹åæˆ–æ˜æ˜¾çš„å®ä½“æå–å…³é”®è¯ï¼Œå¼ºåˆ¶ç”¨æå–æ¨¡å‹
            if extraction_model in check_text or any(word in check_text.lower() for word in extraction_keywords):
                kwargs["model"] = extraction_model
                kwargs["task"] = "extract"
                return await simple_ollama_model_func(prompt, system_prompt, history_messages, **kwargs)
            # åªæœ‰æ˜ç¡®æ˜¯é—®ç­”/ç”Ÿæˆæ‰ç”¨answeræ¨¡å‹
            if answer_model in check_text or any(word in check_text.lower() for word in answer_keywords):
                kwargs["model"] = answer_model
                kwargs["task"] = "answer"
                return await simple_ollama_model_func(prompt, system_prompt, history_messages, **kwargs)
            # é»˜è®¤ç”¨æå–æ¨¡å‹ï¼Œä¿è¯æŠ½å–é˜¶æ®µä¸ä¼šè¯¯ç”¨answeræ¨¡å‹
            kwargs["model"] = extraction_model
            kwargs["task"] = "extract"
            return await simple_ollama_model_func(prompt, system_prompt, history_messages, **kwargs)

        self.rag = LightRAG(
            working_dir=reports_dir,
            workspace="",  # é¿å…é¢å¤–åµŒå¥—
            llm_model_func=llm_model_func_with_task,
            llm_model_name=ANSWER_MODEL,  # é»˜è®¤ä½¿ç”¨é«˜è´¨é‡å›ç­”æ¨¡å‹
            llm_model_max_token_size=8192,
            llm_model_kwargs={
                "host": OLLAMA_HOST,
                "options": {"num_ctx": 8192},
                "timeout": TIMEOUT,
            },
            rerank_model_func=ollama_rerank_func,  # ä½¿ç”¨é…ç½®çš„é‡æ’åºå‡½æ•°
            embedding_func=EmbeddingFunc(
                embedding_dim=embedding_dim,
                max_token_size=8192,
                func=lambda texts: ollama_embed(
                    texts,
                    embed_model=EMBEDDING_MODEL,
                    host=OLLAMA_HOST,
                ),
            ),
        )
        
        await self.rag.initialize_storages()
        await initialize_pipeline_status()
        
        if cleanup_old_data:
            logger.info("RAGç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
        else:
            logger.info("RAGç³»ç»Ÿåˆå§‹åŒ–å®Œæˆï¼ˆä¿ç•™å·²æœ‰æ•°æ®ï¼‰")
    
    async def check_existing_data(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦å·²æœ‰å¤„ç†å¥½çš„æ•°æ®ï¼Œå¯ä»¥ç›´æ¥å¼€å§‹æŸ¥è¯¢"""
        try:
            # ä½¿ç”¨ç®€åŒ–çš„ç›®å½•ç»“æ„
            reports_dir = os.path.join(self.working_dir, "knowledge_base")
            
            # æ£€æŸ¥å…³é”®çš„çŸ¥è¯†å›¾è°±æ–‡ä»¶
            required_files = [
                "kv_store_full_docs.json",
                "kv_store_text_chunks.json", 
                "vdb_entities.json",
                "vdb_relationships.json"
            ]
            
            existing_files = []
            for file_name in required_files:
                file_path = os.path.join(reports_dir, file_name)
                if os.path.exists(file_path):
                    size = os.path.getsize(file_path)
                    if size > 10:  # æ–‡ä»¶ä¸ä¸ºç©ºï¼ˆå¤§äº10å­—èŠ‚ï¼‰
                        existing_files.append((file_name, size))
                        logger.info(f"âœ… å‘ç°å·²å­˜åœ¨æ•°æ®: {file_name} (å¤§å°: {size} bytes)")
            
            # æ£€æŸ¥è¾“å‡ºç›®å½•ä¸­çš„å¤„ç†ç»“æœ
            output_files = []
            if os.path.exists(OUTPUT_DIR):
                for file in os.listdir(OUTPUT_DIR):
                    if file.endswith(('.md', '.json')) and os.path.getsize(os.path.join(OUTPUT_DIR, file)) > 0:
                        output_files.append(file)
                        logger.info(f"âœ… å‘ç°è¾“å‡ºæ–‡ä»¶: {file}")
            
            # å¦‚æœæœ‰è¶³å¤Ÿçš„æ•°æ®æ–‡ä»¶å­˜åœ¨ï¼Œè¯´æ˜ä¹‹å‰å·²ç»å¤„ç†è¿‡æ–‡æ¡£
            if len(existing_files) >= 2:  # è‡³å°‘æœ‰2ä¸ªå…³é”®æ–‡ä»¶å­˜åœ¨
                logger.info(f"ğŸ“š å‘ç°å·²å­˜åœ¨çš„çŸ¥è¯†åº“æ•°æ® ({len(existing_files)}/{len(required_files)} ä¸ªæ–‡ä»¶)")
                if output_files:
                    logger.info(f"ğŸ“„ å‘ç° {len(output_files)} ä¸ªå¤„ç†ç»“æœæ–‡ä»¶")
                return True
            else:
                logger.info("âŒ æœªå‘ç°å®Œæ•´çš„çŸ¥è¯†åº“æ•°æ®")
                return False
                
        except Exception as e:
            logger.error(f"æ£€æŸ¥å·²å­˜åœ¨æ•°æ®å¤±è´¥: {e}")
            return False
    
    async def load_existing_data(self) -> bool:
        """åŠ è½½å·²å­˜åœ¨çš„æ•°æ®åˆ°RAGç³»ç»Ÿ"""
        try:
            # åˆå§‹åŒ–RAGç³»ç»Ÿï¼ˆä¸æ¸…ç†æ•°æ®ï¼‰
            await self._initialize_rag(cleanup_old_data=False)
            
            # æ£€æŸ¥çŸ¥è¯†å›¾è°±çŠ¶æ€
            await self._check_knowledge_graph_status()
            
            logger.info("âœ… å·²æˆåŠŸåŠ è½½ç°æœ‰æ•°æ®åˆ°RAGç³»ç»Ÿ")
            return True
            
        except Exception as e:
            logger.error(f"åŠ è½½å·²å­˜åœ¨æ•°æ®å¤±è´¥: {e}")
            return False
    

    async def _check_knowledge_graph_status(self):
        """æ£€æŸ¥çŸ¥è¯†å›¾è°±æ„å»ºçŠ¶æ€"""
        try:
            # ä½¿ç”¨ç®€åŒ–çš„ç›®å½•ç»“æ„
            reports_dir = os.path.join(self.working_dir, "knowledge_base")
            
            # æ£€æŸ¥å…³é”®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            kg_files = {
                "graph_chunk_entity_relation.graphml": "çŸ¥è¯†å›¾è°±æ–‡ä»¶",
                "vdb_entities.json": "å®ä½“å‘é‡æ•°æ®åº“",
                "vdb_relationships.json": "å…³ç³»å‘é‡æ•°æ®åº“",
                "kv_store_full_docs.json": "æ–‡æ¡£å­˜å‚¨",
                "kv_store_text_chunks.json": "æ–‡æœ¬å—å­˜å‚¨"
            }
            
            for file_name, description in kg_files.items():
                file_path = os.path.join(reports_dir, file_name)
                if os.path.exists(file_path):
                    size = os.path.getsize(file_path)
                    logger.info(f"âœ… {description}: {file_path} (å¤§å°: {size} bytes)")
                else:
                    logger.warning(f"âŒ {description}: {file_path} ä¸å­˜åœ¨")
                    
        except Exception as e:
            logger.error(f"æ£€æŸ¥çŸ¥è¯†å›¾è°±çŠ¶æ€å¤±è´¥: {e}")

    
    def cleanup_all_output_files(self):
        """
        æ¸…ç† financial_output ç›®å½•ä¸‹æ‰€æœ‰æ— ç”¨ .md æ–‡ä»¶å’Œç©ºæ–‡ä»¶å¤¹ï¼ˆé€’å½’å¤„ç†ï¼‰
        - åªä¿ç•™ *_enhanced.md æ–‡ä»¶ï¼Œå…¶ä½™ .md ä¸€å¾‹åˆ é™¤
        - é€’å½’åˆ é™¤æ‰€æœ‰ç©ºæ–‡ä»¶å¤¹
        - å¤„ç†éšè—æ–‡ä»¶å’Œ .DS_Store
        """
        import shutil
        if not os.path.exists(OUTPUT_DIR):
            logger.info("è¾“å‡ºç›®å½•ä¸å­˜åœ¨ï¼Œæ— éœ€æ¸…ç†")
            return
        logger.info("ğŸ§¹ å¼€å§‹é€’å½’æ¸…ç† financial_output ç›®å½•...")
        cleaned_files = []
        cleaned_dirs = []

        # é€’å½’åˆ é™¤æ‰€æœ‰é _enhanced.md çš„ .md æ–‡ä»¶
        for root, dirs, files in os.walk(OUTPUT_DIR):
            for fname in files:
                fpath = os.path.join(root, fname)
                # åˆ é™¤æ‰€æœ‰ .md ä¸”ä¸æ˜¯ _enhanced.md çš„æ–‡ä»¶
                if fname.endswith('.md') and not fname.endswith('_enhanced.md'):
                    try:
                        os.remove(fpath)
                        cleaned_files.append(fpath)
                        logger.info(f"ğŸ§¹ åˆ é™¤è€çš„ markdown æ–‡ä»¶: {fpath}")
                    except Exception as e:
                        logger.warning(f"åˆ é™¤ {fpath} å¤±è´¥: {e}")
                # åˆ é™¤ macOS ä¸‹çš„ .DS_Store
                elif fname == '.DS_Store':
                    try:
                        os.remove(fpath)
                        cleaned_files.append(fpath)
                        logger.info(f"ğŸ§¹ åˆ é™¤æ— ç”¨æ–‡ä»¶: {fpath}")
                    except Exception as e:
                        logger.warning(f"åˆ é™¤ {fpath} å¤±è´¥: {e}")

        # é€’å½’åˆ é™¤æ‰€æœ‰ç©ºæ–‡ä»¶å¤¹ï¼ˆè‡ªåº•å‘ä¸Šï¼‰
        for root, dirs, files in os.walk(OUTPUT_DIR, topdown=False):
            for d in dirs:
                dpath = os.path.join(root, d)
                # è·³è¿‡éšè—æ–‡ä»¶å¤¹
                if d.startswith('.'):
                    continue
                try:
                    # åªè¦ç›®å½•ä¸ºç©ºå°±åˆ 
                    if not os.listdir(dpath):
                        os.rmdir(dpath)
                        cleaned_dirs.append(dpath)
                        logger.info(f"ğŸ§¹ åˆ é™¤ç©ºæ–‡ä»¶å¤¹: {dpath}")
                except Exception as e:
                    logger.warning(f"åˆ é™¤ç©ºæ–‡ä»¶å¤¹ {dpath} å¤±è´¥: {e}")

        if cleaned_files or cleaned_dirs:
            logger.info(f"âœ… æ¸…ç†å®Œæˆ: åˆ é™¤äº† {len(cleaned_files)} ä¸ªæ–‡ä»¶å’Œ {len(cleaned_dirs)} ä¸ªç©ºæ–‡ä»¶å¤¹")
        else:
            logger.info("âœ… æ— éœ€æ¸…ç†ï¼Œç›®å½•å·²å¹²å‡€")
    
    
    async def process_markdown_with_image_descriptions(self, md_content: str, image_dir: str) -> str:
        """å¤„ç†markdownå†…å®¹ï¼Œå°†å›¾ç‰‡æ›¿æ¢ä¸ºæ–‡å­—æè¿°"""
        
        def replace_image_with_description(match):
            image_path = match.group(1)
            # å¦‚æœè·¯å¾„ä»¥"images/"å¼€å¤´ï¼Œåˆ™ç§»é™¤å®ƒï¼Œå› ä¸ºimage_dirå·²ç»æ˜¯imagesç›®å½•äº†
            if image_path.startswith("images/"):
                image_filename = image_path[7:]  # ç§»é™¤"images/"å‰ç¼€
            else:
                image_filename = image_path
            
            full_image_path = os.path.join(image_dir, image_filename)
            
            if os.path.exists(full_image_path):
                try:
                    # ç›´æ¥ä½¿ç”¨Ollamaæ¨¡å‹æè¿°å›¾ç‰‡ï¼Œä¸é€šè¿‡LightRAG
                    description = self.image_describer.describe_image_sync(full_image_path)
                    # æ ¼å¼åŒ–æè¿°æ–‡æœ¬
                    return f"\n**[å›¾ç‰‡æè¿°]**: {description}\n"
                except Exception as e:
                    logger.error(f"å¤„ç†å›¾ç‰‡æè¿°å¤±è´¥ {image_filename}: {e}")
                    return f"\n**[å›¾ç‰‡æè¿°å¤±è´¥]**: {image_filename}\n"
            else:
                logger.warning(f"Image file not found: {full_image_path}")
                return f"\n**[å›¾ç‰‡æœªæ‰¾åˆ°]**: {image_path}\n"
        
        # åŒ¹é…markdownä¸­çš„å›¾ç‰‡è¯­æ³• ![alt](path)
        image_pattern = r'!\[.*?\]\((.*?)\)'
        processed_content = re.sub(image_pattern, replace_image_with_description, md_content)
        
        return processed_content
    
    async def _process_images_with_vision_model(self, md_content: str, image_dir: str, file_name: str) -> str:
        """ä½¿ç”¨è§†è§‰æ¨¡å‹å¤„ç†å›¾ç‰‡å¹¶ç”Ÿæˆæè¿°"""
        if not os.path.exists(image_dir):
            logger.warning(f"å›¾ç‰‡ç›®å½•ä¸å­˜åœ¨: {image_dir}")
            return md_content
            
        logger.info(f"å¼€å§‹å¤„ç†å›¾ç‰‡ç›®å½•: {image_dir}")
        
        # ä½¿ç”¨pdf_batch_processor.pyçš„å¤„ç†æ–¹æ³•
        try:
            enhanced_content = await self.process_markdown_with_image_descriptions(md_content, image_dir)
            logger.info(f"å·²å®Œæˆå›¾ç‰‡æè¿°å¤„ç†")
            return enhanced_content
        except Exception as e:
            logger.error(f"å›¾ç‰‡æè¿°å¤„ç†å¤±è´¥: {e}")
            return md_content
    
    async def _describe_image_with_vision_model(self, image_path: str) -> str:
        """ä½¿ç”¨è§†è§‰æ¨¡å‹æè¿°å•ä¸ªå›¾ç‰‡"""
        try:
            # ä½¿ç”¨ç‹¬ç«‹çš„å›¾ç‰‡æè¿°å™¨ï¼Œé¿å…LightRAGçš„hashing_kvé—®é¢˜
            description = await self.image_describer.describe_image(image_path)
            
            if description and not description.startswith("[å›¾ç‰‡") and not description.startswith("å›¾ç‰‡åˆ†æå¤±è´¥"):
                return description.strip()
            else:
                return f"[æ— æ³•ç”Ÿæˆå›¾ç‰‡æè¿°: {os.path.basename(image_path)}]"
                
        except Exception as e:
            logger.error(f"è§†è§‰æ¨¡å‹æè¿°å›¾ç‰‡å¤±è´¥ {image_path}: {e}")
            return f"[å›¾ç‰‡æè¿°å¤±è´¥: {os.path.basename(image_path)}]"
    
    async def process_document(self, file_path: str) -> bool:
        """å¤„ç†æ–‡æ¡£"""
        try:
            logger.info(f"å¼€å§‹å¤„ç†æ–‡æ¡£: {file_path}")
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not os.path.exists(file_path):
                logger.error(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                return False
            # PDF+MinerUæµç¨‹
            if file_path.lower().endswith('.pdf') and HAS_MINERU:
                try:
                    logger.info("ä½¿ç”¨ MinerU ç›´æ¥å¤„ç† PDF æ–‡ä»¶...")
                    from mineru.cli.common import convert_pdf_bytes_to_bytes_by_pypdfium2, prepare_env, read_fn
                    from mineru.data.data_reader_writer import FileBasedDataWriter
                    from mineru.backend.pipeline.pipeline_analyze import doc_analyze as pipeline_doc_analyze
                    from mineru.backend.pipeline.pipeline_middle_json_mkcontent import union_make as pipeline_union_make
                    from mineru.backend.pipeline.model_json_to_middle_json import result_to_middle_json as pipeline_result_to_middle_json
                    from mineru.utils.enum_class import MakeMode
                    
                    # è¯»å–PDFæ–‡ä»¶
                    file_name = str(Path(file_path).stem)
                    pdf_bytes = read_fn(file_path)
                    
                    # è½¬æ¢PDFå­—èŠ‚
                    new_pdf_bytes = convert_pdf_bytes_to_bytes_by_pypdfium2(pdf_bytes, 0, None)
                    
                    # ä½¿ç”¨pipelineæ¨¡å¼åˆ†æ
                    infer_results, all_image_lists, all_pdf_docs, lang_list, ocr_enabled_list = pipeline_doc_analyze(
                        [new_pdf_bytes], ['ch'], parse_method="auto", formula_enable=True, table_enable=True
                    )
                    
                    # å‡†å¤‡è¾“å‡ºç¯å¢ƒ
                    local_image_dir, local_md_dir = prepare_env(OUTPUT_DIR, file_name, "auto")
                    image_writer, md_writer = FileBasedDataWriter(local_image_dir), FileBasedDataWriter(local_md_dir)
                    
                    # å¤„ç†ç»“æœ
                    model_list = infer_results[0]
                    images_list = all_image_lists[0]
                    pdf_doc = all_pdf_docs[0]
                    _lang = lang_list[0]
                    _ocr_enable = ocr_enabled_list[0]
                    
                    # è½¬æ¢ä¸ºä¸­é—´JSONæ ¼å¼
                    middle_json = pipeline_result_to_middle_json(
                        model_list, images_list, pdf_doc, image_writer, _lang, _ocr_enable, True
                    )
                    
                    pdf_info = middle_json["pdf_info"]
                    
                    # ç”Ÿæˆmarkdownå†…å®¹
                    image_dir = str(os.path.basename(local_image_dir))
                    md_content_str = pipeline_union_make(pdf_info, MakeMode.MM_MD, image_dir)
                    
                    # ä¿å­˜ç”Ÿæˆçš„Markdownåˆ°æ–‡ä»¶
                    md_output_path = os.path.join(OUTPUT_DIR, f"{file_name}.md")
                    with open(md_output_path, 'w', encoding='utf-8') as f:
                        f.write(md_content_str)
                    logger.info(f"Markdownå†…å®¹å·²ä¿å­˜åˆ°: {md_output_path}")
                    
                    # å¤„ç†å›¾ç‰‡ï¼šä½¿ç”¨Ollamaç›´æ¥è§£æå›¾ç‰‡å¹¶æ›¿æ¢markdownä¸­çš„å›¾ç‰‡é“¾æ¥
                    logger.info("å¼€å§‹ä½¿ç”¨Ollamaå¤„ç†å›¾ç‰‡...")
                    enhanced_content = await self._process_images_with_vision_model(
                        md_content_str, local_image_dir, file_name
                    )
                    enhanced_md_path = os.path.join(OUTPUT_DIR, f"{file_name}_enhanced.md")
                    with open(enhanced_md_path, 'w', encoding='utf-8') as f:
                        f.write(enhanced_content)
                    logger.info(f"å¢å¼ºç‰ˆMarkdownå·²ä¿å­˜åˆ°: {enhanced_md_path}")
                    await self.rag.ainsert(enhanced_content)
                    logger.info("âœ… å›¾ç‰‡å¤„ç†å®Œæˆï¼Œå†…å®¹å·²æ’å…¥åˆ°LightRAG")
                    logger.info("â³ ç­‰å¾…çŸ¥è¯†å›¾è°±æ„å»ºå®Œæˆ...")
                    await asyncio.sleep(3)
                    await self._check_knowledge_graph_status()
                    
                    # æ¸…ç†å›¾åƒç›®å½•
                    try:
                        if os.path.exists(local_image_dir):
                            shutil.rmtree(local_image_dir)
                            logger.info(f"Processed images with Ollama and cleaned up {local_image_dir}")
                    except Exception as e:
                        logger.error(f"Failed to clean up image directory {local_image_dir}: {e}")
                    try:
                        if os.path.exists(local_md_dir) and local_md_dir != local_image_dir:
                            md_files = [f for f in os.listdir(local_md_dir) if not f.startswith('.') and os.path.isfile(os.path.join(local_md_dir, f))]
                            if md_files:
                                for md_file in md_files:
                                    try:
                                        os.remove(os.path.join(local_md_dir, md_file))
                                    except Exception:
                                        pass
                                logger.info(f"å·²æ¸…ç†Markdownä¸´æ—¶æ–‡ä»¶: {local_md_dir}")
                    except Exception as e:
                        logger.warning(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {e}")
                    
                    logger.info("âœ… ä½¿ç”¨ MinerU ç›´æ¥å¤„ç† PDF å®Œæˆ")
                    return True
                    
                except Exception as mineru_error:
                    logger.error(f"MinerU ç›´æ¥å¤„ç†å¤±è´¥: {mineru_error}")
                    return False
            logger.warning("âŒ ä»…æ”¯æŒPDF+MinerUæµç¨‹ï¼Œå…¶ä»–ç±»å‹æ–‡ä»¶æš‚ä¸å¤„ç†")
            return False
        except Exception as e:
            logger.error(f"æ–‡æ¡£å¤„ç†å¤±è´¥: {e}")
            return False
    
    async def query_document(self, question: str) -> str:
        """æŸ¥è¯¢æ–‡æ¡£ - ä½¿ç”¨ hybrid æ¨¡å¼å’Œæµå¼è¾“å‡º"""
        try:
            logger.info(f"æŸ¥è¯¢é—®é¢˜: {question}")
            
            # ä½¿ç”¨ hybrid æ¨¡å¼å’Œæµå¼è¾“å‡º
            query_param = QueryParam(
                mode="hybrid",
                stream=True,
            )
            
            # æ‰§è¡ŒæŸ¥è¯¢
            response = await self.rag.aquery(question, param=query_param)
            
            # å¤„ç†æµå¼å“åº”
            if inspect.isasyncgen(response):
                result_chunks = []
                async for chunk in response:
                    result_chunks.append(chunk)
                response = "".join(result_chunks)
            
            # æ£€æŸ¥å“åº”æ˜¯å¦ä¸ºç©º
            if not response or "æŠ±æ­‰ï¼Œæ‚¨æ²¡æœ‰æä¾›æ–‡ç« å†…å®¹" in response:
                return "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•åœ¨å½“å‰æ–‡æ¡£ä¸­æ‰¾åˆ°ç›¸å…³ä¿¡æ¯æ¥å›ç­”æ‚¨çš„é—®é¢˜ã€‚è¯·å°è¯•è¯¢é—®ä¸æ–‡æ¡£å†…å®¹æ›´ç›¸å…³çš„é—®é¢˜ã€‚"
            
            logger.info("æŸ¥è¯¢å®Œæˆ")
            return response
            
        except Exception as e:
            logger.error(f"æŸ¥è¯¢å¤±è´¥: {e}")
            return f"æŸ¥è¯¢å‡ºé”™: {str(e)}"
    
    async def query_with_financial_context(self, question: str, context_data: Optional[Dict] = None) -> str:
        """é‡‘èæŸ¥è¯¢"""
        try:
            # ä½¿ç”¨æœ¬åœ°æç¤ºè¯æ–‡ä»¶ä¸­çš„é‡‘èç³»ç»Ÿæç¤º
            financial_system_prompt = local_prompt.PROMPTS["financial_system"]
            
            if context_data and HAS_RAGANYTHING:
                # å¦‚æœæœ‰é¢å¤–çš„è´¢åŠ¡æ•°æ®ï¼Œä½¿ç”¨å¤šæ¨¡æ€æŸ¥è¯¢
                multimodal_content = []
                
                # æ·»åŠ è¡¨æ ¼æ•°æ®
                if "table_data" in context_data:
                    multimodal_content.append({
                        "type": "table",
                        "table_data": context_data["table_data"],
                        "table_caption": context_data.get("table_caption", "è´¢åŠ¡æ•°æ®è¡¨")
                    })
                
                # æ·»åŠ å…¬å¼
                if "formula" in context_data:
                    multimodal_content.append({
                        "type": "equation", 
                        "latex": context_data["formula"],
                        "equation_caption": context_data.get("formula_caption", "è´¢åŠ¡è®¡ç®—å…¬å¼")
                    })
                
                if multimodal_content:
                    # ä½¿ç”¨å¤šæ¨¡æ€æŸ¥è¯¢
                    enhanced_question = f"{financial_system_prompt}\n\n{question}"
                    # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦RAGAnythingå®ä¾‹ï¼Œå®é™…å®ç°æ—¶éœ€è¦è°ƒæ•´
                    logger.info("ä½¿ç”¨å¤šæ¨¡æ€æŸ¥è¯¢å¤„ç†é‡‘èé—®é¢˜")
            
            # æ ‡å‡†æŸ¥è¯¢æµç¨‹
            response = await self.query_document(question)
            
            return response
            
        except Exception as e:
            logger.error(f"é‡‘èä¸“ä¸šæŸ¥è¯¢å¤±è´¥: {e}")
            return f"é‡‘èæŸ¥è¯¢å‡ºé”™: {str(e)}"
    
    async def process_documents_batch(self, report_directory: str = None) -> Dict[str, bool]:
        """å¢é‡æ‰¹é‡å¤„ç†æŠ¥å‘Šæ–‡ä»¶å¤¹ä¸‹çš„æ‰€æœ‰æ–‡æ¡£"""
        if report_directory is None:
            # é»˜è®¤ä½¿ç”¨åŒçº§çš„Reportæ–‡ä»¶å¤¹
            current_dir = os.path.dirname(os.path.abspath(__file__))
            report_directory = os.path.join(current_dir, "Report")
        
        logger.info(f"å¼€å§‹æ‰¹é‡å¤„ç†æŠ¥å‘Šæ–‡ä»¶å¤¹: {report_directory}")
        
        # æ£€æŸ¥Reportæ–‡ä»¶å¤¹æ˜¯å¦å­˜åœ¨
        if not os.path.exists(report_directory):
            logger.error(f"æŠ¥å‘Šæ–‡ä»¶å¤¹ä¸å­˜åœ¨: {report_directory}")
            return {}
        
        # é€’å½’æ”¶é›†æ‰€æœ‰æ”¯æŒçš„æ–‡ä»¶
        supported_extensions = {'.pdf', '.txt', '.md'}
        all_files = []
        
        for root, dirs, files in os.walk(report_directory):
            for file in files:
                file_path = os.path.join(root, file)
                file_ext = os.path.splitext(file)[1].lower()
                if file_ext in supported_extensions:
                    all_files.append(file_path)
        
        logger.info(f"å‘ç° {len(all_files)} ä¸ªå¯å¤„ç†çš„æ–‡ä»¶")
        
        # æ£€æŸ¥å“ªäº›æ–‡ä»¶å·²ç»å¤„ç†è¿‡ï¼ˆå¢é‡å¤„ç†é€»è¾‘ï¼‰
        processed_files = set()
        new_files = []
        
        for file_path in all_files:
            file_name = Path(file_path).stem
            enhanced_md_path = os.path.join(OUTPUT_DIR, f"{file_name}_enhanced.md")
            simple_md_path = os.path.join(OUTPUT_DIR, f"{file_name}.md")
            
            # å¦‚æœå­˜åœ¨å¢å¼ºç‰ˆæˆ–ç®€å•ç‰ˆçš„markdownæ–‡ä»¶ï¼Œè®¤ä¸ºå·²å¤„ç†
            if os.path.exists(enhanced_md_path) or os.path.exists(simple_md_path):
                processed_files.add(file_path)
                logger.info(f"â­ï¸  æ–‡ä»¶å·²å¤„ç†ï¼Œè·³è¿‡: {os.path.basename(file_path)}")
            else:
                new_files.append(file_path)
        
        logger.info(f"éœ€è¦å¤„ç†çš„æ–°æ–‡ä»¶: {len(new_files)} ä¸ª")
        logger.info(f"å·²è·³è¿‡çš„æ–‡ä»¶: {len(processed_files)} ä¸ª")
        
        if not new_files:
            logger.info("âœ… æ‰€æœ‰æ–‡ä»¶éƒ½å·²å¤„ç†å®Œæˆ")
            return {file_path: True for file_path in all_files}
        
        # æŒ‰é¡ºåºå¤„ç†æ–°æ–‡ä»¶ï¼ˆéå¼‚æ­¥ï¼Œé¿å…èµ„æºå†²çªï¼‰
        results = {}
        processed_count = 0
        failed_count = 0
        
        # å…ˆå°†å·²å¤„ç†çš„æ–‡ä»¶æ ‡è®°ä¸ºæˆåŠŸ
        for file_path in processed_files:
            results[file_path] = True
        
        for i, file_path in enumerate(new_files, 1):
            logger.info(f"å¤„ç†è¿›åº¦: {i}/{len(new_files)} - {os.path.basename(file_path)}")
            
            try:
                # å¤„ç†å•ä¸ªæ–‡æ¡£
                success = await self.process_document(file_path)
                results[file_path] = success
                
                if success:
                    processed_count += 1
                    logger.info(f"âœ… æˆåŠŸå¤„ç†: {os.path.basename(file_path)}")
                else:
                    failed_count += 1
                    logger.error(f"âŒ å¤„ç†å¤±è´¥: {os.path.basename(file_path)}")
                
                # åœ¨æ–‡æ¡£ä¹‹é—´æ·»åŠ çŸ­æš‚å»¶è¿Ÿï¼Œé¿å…èµ„æºå†²çª
                if i < len(new_files):
                    logger.info("â³ ç­‰å¾…3ç§’åå¤„ç†ä¸‹ä¸€ä¸ªæ–‡ä»¶...")
                    await asyncio.sleep(3)
                    
            except Exception as e:
                logger.error(f"å¤„ç†æ–‡æ¡£æ—¶å‘ç”Ÿå¼‚å¸¸ {os.path.basename(file_path)}: {e}")
                results[file_path] = False
                failed_count += 1
        
        # æœ€ç»ˆç»Ÿè®¡
        total_processed = len(processed_files) + processed_count
        total_files = len(all_files)
        
        logger.info(f"æ‰¹é‡å¤„ç†å®Œæˆ!")
        logger.info(f"  æ€»æ–‡ä»¶æ•°: {total_files}")
        logger.info(f"  å·²å¤„ç†(è·³è¿‡): {len(processed_files)}")
        logger.info(f"  æ–°å¤„ç†æˆåŠŸ: {processed_count}")
        logger.info(f"  å¤„ç†å¤±è´¥: {failed_count}")
        logger.info(f"  æ•´ä½“æˆåŠŸç‡: {total_processed}/{total_files} ({total_processed/total_files*100:.1f}%)")
        
        return results
    
    async def process_single_file_to_knowledge_base(self, file_path: str) -> bool:
        """å¤„ç†å•ä¸ªæ–‡ä»¶å¹¶æ·»åŠ åˆ°çŸ¥è¯†åº“ï¼ˆç”¨äºå¢é‡æ·»åŠ ï¼‰"""
        try:
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²ç»å¤„ç†è¿‡
            file_name = Path(file_path).stem
            enhanced_md_path = os.path.join(OUTPUT_DIR, f"{file_name}_enhanced.md")
            simple_md_path = os.path.join(OUTPUT_DIR, f"{file_name}.md")
            
            if os.path.exists(enhanced_md_path) or os.path.exists(simple_md_path):
                logger.info(f"æ–‡ä»¶å·²å¤„ç†ï¼Œç›´æ¥åŠ è½½åˆ°çŸ¥è¯†åº“: {os.path.basename(file_path)}")
                
                # è¯»å–å·²å¤„ç†çš„markdownå†…å®¹
                content_path = enhanced_md_path if os.path.exists(enhanced_md_path) else simple_md_path
                with open(content_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # ç›´æ¥æ’å…¥åˆ°çŸ¥è¯†åº“
                await self.rag.ainsert(content)
                logger.info(f"âœ… å·²å°†å¤„ç†è¿‡çš„å†…å®¹æ·»åŠ åˆ°çŸ¥è¯†åº“: {os.path.basename(file_path)}")
                return True
            else:
                # å¤„ç†æ–°æ–‡ä»¶
                logger.info(f"å¤„ç†æ–°æ–‡ä»¶: {os.path.basename(file_path)}")
                return await self.process_document(file_path)
                
        except Exception as e:
            logger.error(f"å•æ–‡ä»¶å¤„ç†å¤±è´¥ {file_path}: {e}")
            return False
    
    async def update_knowledge_base_incrementally(self, report_directory: str = None) -> Dict[str, bool]:
        """å¢é‡æ›´æ–°çŸ¥è¯†åº“"""
        import re
        if report_directory is None:
            current_dir = os.path.dirname(os.path.abspath(__file__))
            report_directory = os.path.join(current_dir, "Report")

        logger.info(f"å¢é‡æ›´æ–°çŸ¥è¯†åº“ï¼Œæ‰«æç›®å½•: {report_directory}")

        # 1. è·å– financial_output ç›®å½•ä¸‹æ‰€æœ‰å·²å¤„ç†æ–‡ä»¶çš„æ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼Œå«_enhancedï¼‰
        processed_names = set()
        if os.path.exists(OUTPUT_DIR):
            for fname in os.listdir(OUTPUT_DIR):
                if fname.endswith('.md'):
                    # æ”¯æŒ _enhanced.md å’Œ .md
                    name = fname[:-3]  # å»æ‰.md
                    if name.endswith('_enhanced'):
                        name = name[:-9]  # å»æ‰_enhanced
                    processed_names.add(name)

        # 2. éå† Report åŠå…¶å­ç›®å½•ï¼Œæ”¶é›†æ‰€æœ‰æ”¯æŒçš„æ–‡ä»¶
        supported_extensions = {'.pdf', '.txt', '.md'}
        all_files = []
        file_name_map = {}  # stem -> full path
        for root, dirs, files in os.walk(report_directory):
            for file in files:
                file_ext = os.path.splitext(file)[1].lower()
                if file_ext in supported_extensions:
                    file_path = os.path.join(root, file)
                    stem = Path(file).stem
                    all_files.append(file_path)
                    file_name_map[stem] = file_path

        if not all_files:
            logger.warning(f"åœ¨ç›®å½• {report_directory} ä¸­æœªæ‰¾åˆ°å¯å¤„ç†çš„æ–‡ä»¶")
            return {}

        # 3. åªå¤„ç†æœªåœ¨ processed_names ä¸­çš„æ–‡ä»¶
        new_files = []
        for file_path in all_files:
            stem = Path(file_path).stem
            if stem not in processed_names:
                new_files.append(file_path)

        logger.info(f"å‘ç° {len(new_files)} ä¸ªéœ€è¦å¢é‡å¤„ç†çš„æ–°æ–‡ä»¶ï¼Œ{len(all_files)-len(new_files)} ä¸ªå·²å¤„ç†æ–‡ä»¶å°†è·³è¿‡ã€‚")

        results = {}
        new_processed = 0
        failed = 0
        for i, file_path in enumerate(new_files, 1):
            logger.info(f"å¢é‡å¤„ç†è¿›åº¦: {i}/{len(new_files)} - {os.path.basename(file_path)}")
            try:
                success = await self.process_document(file_path)
                results[file_path] = success
                if success:
                    new_processed += 1
                    logger.info(f"âœ… æ–°å¤„ç†æ–‡ä»¶: {os.path.basename(file_path)}")
                else:
                    failed += 1
                    logger.error(f"âŒ å¤„ç†å¤±è´¥: {os.path.basename(file_path)}")
                if i < len(new_files):
                    await asyncio.sleep(1)
            except Exception as e:
                logger.error(f"å¤„ç†æ–‡ä»¶å¼‚å¸¸ {os.path.basename(file_path)}: {e}")
                results[file_path] = False
                failed += 1

        logger.info(f"å¢é‡æ›´æ–°å®Œæˆ! æ–°å¤„ç†: {new_processed}ï¼Œå¤±è´¥: {failed}ï¼Œæ€»æ–°æ–‡ä»¶: {len(new_files)}")
        return results

    async def finalize(self):
        """æ¸…ç†èµ„æº"""
        if self.rag:
            try:
                # ç¡®ä¿æ‰€æœ‰ LightRAG ç¼“å­˜æ“ä½œå·²å®Œæˆ
                if hasattr(self.rag, 'llm_response_cache') and self.rag.llm_response_cache:
                    await self.rag.llm_response_cache.index_done_callback()
                
                # ç»“æŸæ‰€æœ‰å­˜å‚¨
                await self.rag.finalize_storages()
                logger.info("RAGç³»ç»Ÿèµ„æºæ¸…ç†å®Œæˆ")
            except Exception as e:
                logger.error(f"RAGç³»ç»Ÿèµ„æºæ¸…ç†å¤±è´¥: {e}")
        
        # ç­‰å¾…ä¸€æ®µæ—¶é—´ä»¥ç¡®ä¿æ‰€æœ‰å¤„ç†éƒ½å·²å®Œæˆ
        await asyncio.sleep(1)
        
        logger.info("ç¨‹åºç»“æŸ - å›¾ç‰‡ç›®å½•å·²åœ¨å¤„ç†æ—¶æ¸…ç†å®Œæˆ")


def get_local_prompts():
    """è·å–æœ¬åœ°æç¤ºè¯ï¼Œç”¨äºæ›¿æ¢LightRAGåº“ä¸­çš„é»˜è®¤æç¤ºè¯"""
    from lightrag.prompt import PROMPTS as lightrag_prompts
    
    # ç”¨æœ¬åœ°æç¤ºè¯æ›¿æ¢é»˜è®¤æç¤ºè¯
    for key, value in local_prompt.PROMPTS.items():
        lightrag_prompts[key] = value
    
    logger.info(f"å·²åŠ è½½æœ¬åœ°æç¤ºè¯ï¼Œå…± {len(local_prompt.PROMPTS)} ä¸ª")


async def main():
    """ä¸»å‡½æ•°"""
    global MINERU_DEVICE
    
    parser = argparse.ArgumentParser(description="ç ”æŠ¥å¤„ç†å™¨")
    parser.add_argument("file_path", nargs='?', help="è¦å¤„ç†çš„PDFæˆ–æ–‡æœ¬æ–‡ä»¶è·¯å¾„")
    parser.add_argument(
        "--working_dir", "-w", 
        default=WORKING_DIR, 
        help="å·¥ä½œç›®å½•è·¯å¾„"
    )
    parser.add_argument(
        "--query", "-q",
        help="è¦æŸ¥è¯¢çš„é—®é¢˜"
    )
    parser.add_argument(
        "--check-deps",
        action="store_true",
        help="æ£€æŸ¥ä¾èµ–é¡¹å®‰è£…çŠ¶æ€"
    )
    parser.add_argument(
        "--device",
        default=MINERU_DEVICE,
        help=f"MinerU å¤„ç†è®¾å¤‡ (é»˜è®¤: {MINERU_DEVICE})"
    )
    parser.add_argument(
        "--force-reprocess",
        action="store_true",
        help="å¼ºåˆ¶é‡æ–°å¤„ç†æ–‡æ¡£ï¼Œå¿½ç•¥å·²å­˜åœ¨çš„æ•°æ®"
    )
    
    args = parser.parse_args()
    
    # é…ç½®æ—¥å¿—
    configure_logging()
    
    print("=" * 50)
    print("ğŸ¦ ç ”æŠ¥å¤„ç†å™¨")
    print("=" * 50)
    
    # å¦‚æœåªæ˜¯æ£€æŸ¥ä¾èµ–é¡¹
    if args.check_deps:
        print("\nğŸ” ä¾èµ–é¡¹æ£€æŸ¥:")
        print(f"âœ… RAGAnything: {'å·²å®‰è£…' if HAS_RAGANYTHING else 'æœªå®‰è£…'}")
        print(f"âœ… MinerU 2.0: {'å·²å®‰è£…' if HAS_MINERU else 'æœªå®‰è£…'}")
        return
    
    # æ£€æŸ¥æ˜¯å¦æä¾›äº†æ–‡ä»¶è·¯å¾„
    if not args.file_path:
        # å¦‚æœæ²¡æœ‰æä¾›æ–‡ä»¶è·¯å¾„ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰å·²å­˜åœ¨çš„æ•°æ®
        print("ğŸ“‹ æœªæä¾›æ–‡ä»¶è·¯å¾„ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰å·²å¤„ç†çš„æ•°æ®...")

        # åˆ›å»ºä¸´æ—¶å¤„ç†å™¨å®ä¾‹æ¥æ£€æŸ¥æ•°æ®
        temp_processor = FinancialReportProcessor(args.working_dir)
        has_existing_data = await temp_processor.check_existing_data()

        if has_existing_data:
            print("âœ… å‘ç°å·²å¤„ç†çš„æ•°æ®ï¼")
            print("ğŸš€ æ£€æŸ¥å¹¶å¢é‡å¤„ç†æ–°æ–‡ä»¶...")

            # ä½¿ç”¨å·²å­˜åœ¨çš„æ•°æ®åˆå§‹åŒ–å¤„ç†å™¨
            processor = temp_processor
            try:
                await processor.load_existing_data()
                # è‡ªåŠ¨å¢é‡å¤„ç†æ–°æ–‡ä»¶
                update_results = await processor.update_knowledge_base_incrementally()
                if update_results:
                    new_count = sum(1 for v in update_results.values() if v)
                    print(f"ğŸ“¦ å·²å¢é‡å¤„ç†/åŠ è½½ {new_count} ä¸ªæ–‡ä»¶åˆ°çŸ¥è¯†åº“ã€‚")
                else:
                    print("ğŸ“¦ æ²¡æœ‰å‘ç°éœ€è¦å¢é‡å¤„ç†çš„æ–°æ–‡ä»¶ã€‚")
                print("âœ… æ•°æ®åŠ è½½å®Œæˆï¼Œå¯ä»¥å¼€å§‹æŸ¥è¯¢")

                # ç›´æ¥è¿›å…¥äº¤äº’å¼æŸ¥è¯¢æ¨¡å¼
                print("\nğŸ’¬ è¿›å…¥äº¤äº’æ¨¡å¼ (è¾“å…¥ 'quit' é€€å‡º):")
                print("ğŸ’¡ æç¤º: æ‚¨å¯ä»¥è¯¢é—®å…³äºæ–‡æ¡£å†…å®¹çš„ä»»ä½•é—®é¢˜")
                while True:
                    try:
                        question = input("\nè¯·è¾“å…¥é—®é¢˜: ").strip()
                        if question.lower() in ['quit', 'exit', 'é€€å‡º']:
                            break

                        if question:
                            try:
                                response = await processor.query_document(question)
                                print(f"\n{response}")
                            except Exception as query_error:
                                print(f"æŸ¥è¯¢å¤±è´¥: {query_error}")

                    except KeyboardInterrupt:
                        break
                    except Exception as e:
                        print(f"æŸ¥è¯¢å‡ºé”™: {e}")

            except Exception as e:
                print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
            finally:
                await processor.finalize()
                print("\nğŸ‘‹ ç¨‹åºç»“æŸ")
                return
        else:
            print("âŒ é”™è¯¯: è¯·æä¾›è¦å¤„ç†çš„æ–‡ä»¶è·¯å¾„")
            print("ä½¿ç”¨ --help æŸ¥çœ‹å®Œæ•´å¸®åŠ©ä¿¡æ¯")
            print("\nğŸ’¡ ç¤ºä¾‹ç”¨æ³•:")
            print("python financial_report_processor.py document.pdf")
            print("python financial_report_processor.py --check-deps  # æ£€æŸ¥ä¾èµ–é¡¹")
            print("python financial_report_processor.py  # ç›´æ¥æŸ¥è¯¢å·²å¤„ç†çš„æ•°æ®ï¼ˆå¦‚æœå­˜åœ¨ï¼‰")
            print("python financial_report_processor.py document.pdf --force-reprocess  # å¼ºåˆ¶é‡æ–°å¤„ç†")
            return
    
    # æ£€æŸ¥ä¾èµ–é¡¹
    missing_deps = []
    if not HAS_RAGANYTHING:
        missing_deps.append("RAGAnything")
    if not HAS_MINERU:
        missing_deps.append("MinerU 2.0")
    
    if missing_deps:
        print(f"\nâš ï¸  è­¦å‘Š: ç¼ºå°‘å¿…è¦ä¾èµ–é¡¹: {', '.join(missing_deps)}")
        print("æŸäº›åŠŸèƒ½å°†ä¸å¯ç”¨")
        response = input("æ˜¯å¦ç»§ç»­? (y/N): ").strip().lower()
        if response not in ['y', 'yes']:
            return
    
    # æ›´æ–°è®¾å¤‡é…ç½®
    MINERU_DEVICE = args.device
    os.environ["MINERU_DEVICE"] = MINERU_DEVICE
    
    # åˆå§‹åŒ–å¤„ç†å™¨
    processor = FinancialReportProcessor(args.working_dir)
    
    try:
        # åˆå§‹åŒ–ç³»ç»Ÿ
        await processor.initialize()
        
        # é¦–å…ˆæ£€æŸ¥æ˜¯å¦å·²æœ‰å¤„ç†å¥½çš„æ•°æ®ï¼ˆé™¤éå¼ºåˆ¶é‡æ–°å¤„ç†ï¼‰
        has_existing_data = False
        if not args.force_reprocess:
            has_existing_data = await processor.check_existing_data()
        
        if has_existing_data:
            print("âœ… å‘ç°å·²å¤„ç†çš„æ•°æ®ï¼Œè·³è¿‡æ–‡æ¡£å¤„ç†æ­¥éª¤")
            print("ğŸš€ ç›´æ¥è¿›å…¥æŸ¥è¯¢æ¨¡å¼...")
            print("ğŸ’¡ æç¤º: å¦‚éœ€é‡æ–°å¤„ç†æ–‡æ¡£ï¼Œè¯·ä½¿ç”¨ --force-reprocess å‚æ•°")
            
            # é‡æ–°åˆå§‹åŒ–ä»¥åŠ è½½å·²æœ‰æ•°æ®ï¼ˆä¸æ¸…ç†ï¼‰
            await processor.load_existing_data()
        else:
            if args.force_reprocess:
                print("ğŸ”„ å¼ºåˆ¶é‡æ–°å¤„ç†æ¨¡å¼ï¼Œå°†æ¸…ç†å·²æœ‰æ•°æ®")
            
            # å¤„ç†æ–‡æ¡£
            print(f"\nğŸ“„ æ­£åœ¨å¤„ç†æ–‡æ¡£: {args.file_path}")
            success = await processor.process_document(args.file_path)
            
            if not success:
                print("âŒ æ–‡æ¡£å¤„ç†å¤±è´¥")
                return
            
            print("âœ… æ–‡æ¡£å¤„ç†å®Œæˆ")
        
        # å¦‚æœæä¾›äº†æŸ¥è¯¢é—®é¢˜ï¼Œç›´æ¥æŸ¥è¯¢
        if args.query:
            print(f"\nğŸ’­ æŸ¥è¯¢é—®é¢˜: {args.query}")
            print("ï¿½ ä½¿ç”¨ hybrid æ¨¡å¼å’Œæµå¼è¾“å‡º")
            
            try:
                response = await processor.query_document(args.query)
                print(f"\n{response}")
            except Exception as query_error:
                print(f"æŸ¥è¯¢å¤±è´¥: {query_error}")
        else:
            # äº¤äº’å¼æŸ¥è¯¢
            print("\nğŸ’¬ è¿›å…¥äº¤äº’æ¨¡å¼ (è¾“å…¥ 'quit' é€€å‡º):")
            print("ğŸ’¡ æç¤º: æ‚¨å¯ä»¥è¯¢é—®å…³äºæ–‡æ¡£å†…å®¹çš„ä»»ä½•é—®é¢˜")
            while True:
                try:
                    question = input("\nè¯·è¾“å…¥é—®é¢˜: ").strip()
                    if question.lower() in ['quit', 'exit', 'é€€å‡º']:
                        break
                    
                    if question:
                        try:
                            response = await processor.query_document(question)
                            print(f"\n{response}")
                        except Exception as query_error:
                            print(f"æŸ¥è¯¢å¤±è´¥: {query_error}")
                        
                except KeyboardInterrupt:
                    break
                except Exception as e:
                    print(f"æŸ¥è¯¢å‡ºé”™: {e}")
        
    except Exception as e:
        logger.error(f"ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        print(f"âŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
    finally:
        # åªåšèµ„æºæ¸…ç†ï¼Œä¸åšæ–‡ä»¶æ¸…ç†
        await processor.finalize()
        print("\nğŸ‘‹ ç¨‹åºç»“æŸ")


if __name__ == "__main__":
    import asyncio
    asyncio.run(main())
    # ä¸»æµç¨‹ç»“æŸåå•ç‹¬åŒæ­¥æ¸…ç† financial_output ç›®å½•
    try:
        # é‡æ–°å®ä¾‹åŒ–å¤„ç†å™¨ä»¥ç¡®ä¿æœ‰working_dir
        processor = FinancialReportProcessor()
        processor.cleanup_all_output_files()
        print("\nğŸ§¹ å·²è‡ªåŠ¨æ¸…ç† financial_output ç›®å½•çš„æ— ç”¨æ–‡ä»¶")
    except Exception as e:
        print(f"\nâš ï¸ è‡ªåŠ¨æ¸…ç† financial_output ç›®å½•å¤±è´¥: {e}")
